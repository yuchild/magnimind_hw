{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L-OPosLrniwE"
   },
   "source": [
    "### 1. Intutition Behind Recursive Neural Networks (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P2p6yDqtniwL"
   },
   "source": [
    "RNN is useful when we want to predict the next event given a sequence of events.\n",
    "\n",
    "An example of that could be to predict the word that comes after This is an _____.\n",
    "\n",
    "Let's say, in reality, the sentence is This is an example.\n",
    "\n",
    "Traditional text-mining techniques would solve the problem in the following way:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aMn5yvqyniwM"
   },
   "source": [
    "**a.** Encode each word while having an additional index for potential new words:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NT0w4LwcniwM"
   },
   "source": [
    "- This: {1,0,0,0}\n",
    "- is: {0,1,0,0}\n",
    "- an: {0,0,1,0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hdMv7TsAniwN"
   },
   "source": [
    "**b.** Encode the phrase `This is an`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jn6UslNyniwN"
   },
   "source": [
    "- This is an: {1,1,1,0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wHKkCXuDniwN"
   },
   "source": [
    "**c.** Create the training dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g87l3y27niwO"
   },
   "source": [
    "- Input --> {1,1,1,0}\n",
    "- Output --> {0,0,0,1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZ8QflK1niwO"
   },
   "source": [
    "**d.** Build a model with input and output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YvHEn-ZvniwO"
   },
   "source": [
    "One of the major drawbacks of the model is that the input representation does not change in the input sentence; it is either `this is an`, or `an is this`, or `this an is`.\n",
    "However, intuitively, we know that each of the preceding sentences is different and cannot be represented by the same structure mathematically. This calls for having a different architecture, which looks as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KqSHXrsHniwP"
   },
   "source": [
    "![img1.png](attachment:img1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MBrUA_-qniwP"
   },
   "source": [
    "In the architecture above, each of the individual words from the sentence enter into individual box among the input boxes. However the structure of the sentence will be preserved, for example `this` enters the first box, `is` enters second box and `an` enters the third box.\n",
    "\n",
    "The output box at the top will be the output that is `example`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fWp3EeQrniwQ"
   },
   "source": [
    "### 2. Interpreting RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5P7ymulIniwQ"
   },
   "source": [
    "You can think of RNN as a mechanism to hold memory—where the memory is contained within the hidden layer. It can be visualized as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-WYNc487niwQ"
   },
   "source": [
    "![img2.png](attachment:img2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "saELFxp3niwR"
   },
   "source": [
    "The network on the right is an unrolled version of the network on the left. The network on the right takes one input in each time step and extracts the output at each time step. However, if we are interested in the output in the fourth time step, we'll provide input in the previous three time steps and the output of the third time step is the predicted value for fourth time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D0vTOnHWniwR"
   },
   "source": [
    "#### 3. Building an RNN from scratch in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Z1rVurXIH-8k",
    "outputId": "3035068a-a3ad-418a-d3dc-996f1146bea7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 12:05:56.570218: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import SimpleRNN\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vWk2skDBNiat"
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PwreiBXyniwT"
   },
   "source": [
    "Let's consider an example text that looks as follows: `This is an example`.\n",
    "\n",
    "The task at hand is to predict the third word given a sequence of two words.\n",
    "\n",
    "| **Input**      | **Output**   |\n",
    "| :------------- | :----------: |\n",
    "|  This is       | an           |\n",
    "|  is an         | example      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Ngzgl3hniwT"
   },
   "source": [
    "**1.** Let's define the input and output in code, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "3ARkH7KqIACA"
   },
   "outputs": [],
   "source": [
    "#define documents\n",
    "docs = ['this is','is an']\n",
    "# define class labels\n",
    "labels = ['an','example']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eb9QgBqMniwT"
   },
   "source": [
    "**2.** Let's preprocess our dataset so that it can be passed to an RNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "7Ir7kVq9niwU"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counts = Counter()\n",
    "for i,review in enumerate(docs+labels):\n",
    "    counts.update(review.split())\n",
    "words = sorted(counts, key=counts.get, reverse=True)\n",
    "vocab_size=len(words)\n",
    "word_to_int = {word: i for i, word in enumerate(words, 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "o7ORwln4niwU",
    "outputId": "3c36c544-8c95-4b50-c468-2b8f66f8335b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is': 1, 'an': 2, 'this': 3, 'example': 4}\n"
     ]
    }
   ],
   "source": [
    "print(word_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9OApwSDUniwU",
    "outputId": "14af2524-b0f0-472b-d7ea-c1a2791bae59"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom collections import Counter\\nx = Counter({'a':5, 'b':3, 'c':7})\\n\\nOutside of counters, sorting can always be adjusted based on a key function;\\n.sort() and sorted() both take callable that lets you specify a value on which\\nto sort the input sequence; sorted(x, key=x.get, reverse=True) would give you\\nthe same sorting as x.most_common(), but only return the keys, for example:\\n\\nsorted(x, key=x.get, reverse=True)\\n['c', 'a', 'b']\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = sorted(counts, key=counts.get, reverse=True)\n",
    "'''\n",
    "from collections import Counter\n",
    "x = Counter({'a':5, 'b':3, 'c':7})\n",
    "\n",
    "Outside of counters, sorting can always be adjusted based on a key function;\n",
    ".sort() and sorted() both take callable that lets you specify a value on which\n",
    "to sort the input sequence; sorted(x, key=x.get, reverse=True) would give you\n",
    "the same sorting as x.most_common(), but only return the keys, for example:\n",
    "\n",
    "sorted(x, key=x.get, reverse=True)\n",
    "['c', 'a', 'b']\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "hJDKcKl6niwV",
    "outputId": "74d8a4a0-56d8-4db5-c73a-b44aae61f1cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is', 'an', 'this', 'example']\n"
     ]
    }
   ],
   "source": [
    "words = sorted(counts, key=counts.get, reverse=True)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "39iFyCMRniwV",
    "outputId": "c2df1f3e-682f-48c4-92dd-7be018674911"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['is', 'an', 'this', 'example'], 1)\n",
      "(1, 'is')\n",
      "(2, 'an')\n",
      "(3, 'this')\n",
      "(4, 'example')\n"
     ]
    }
   ],
   "source": [
    "print((words, 1))\n",
    "for i, word in enumerate(words,1):\n",
    "      print((i,word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PkNoXbZGniwV"
   },
   "source": [
    "**3.** Modify the input and output words with their corresponding IDs, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "O014n2-5niwV"
   },
   "outputs": [],
   "source": [
    "encoded_docs = []\n",
    "for doc in docs:\n",
    "    encoded_docs.append([word_to_int[word] for word in doc.split()])\n",
    "encoded_labels = []\n",
    "for label in labels:\n",
    "    encoded_labels.append([word_to_int[word] for word in label.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "YWOXLN8UQUkm",
    "outputId": "f3799369-2200-4cea-ce22-a316eea88f53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_docs:  [[3, 1], [1, 2]]\n",
      "encoded_labels:  [[2], [4]]\n"
     ]
    }
   ],
   "source": [
    "print('encoded_docs: ',encoded_docs)\n",
    "print('encoded_labels: ',encoded_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zrXfkqRcniwW"
   },
   "source": [
    "**4.** One additional factor to take care of while encoding the input is the input length. In cases of sentiment analysis, the input text length can vary from one review to another. However, the neural network expects the input size to be fixed. To get around this problem, we perform padding on top of the input. Padding ensures that all inputs are encoded to have a similar length. While the lengths of both examples in our case is 2, in practice, we are very likely to face the scenario of differing lengths between input. In code, we perform padding as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "VPpPFRiWIFK_",
    "outputId": "09714825-a5ad-4496-81eb-339d7a30133c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 1]\n",
      " [1 2]]\n"
     ]
    }
   ],
   "source": [
    "# pad documents to a max length of 2 words\n",
    "max_length = 2\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='pre')\n",
    "print(padded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g9u-_JSsniwX"
   },
   "source": [
    "**5.** The typical processing for outputs is to make them into dummy values, that is, make a one-hot-encoded version of the output labels, which is done as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "QC5QnJtxSUta",
    "outputId": "fecc9314-f748-4d3d-b042-ebdb64c91fd0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# processing the output dataset\n",
    "one_hot_encoded_labels = to_categorical(encoded_labels, num_classes=5) ####Why not have 2-5 classes\n",
    "print(one_hot_encoded_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0isJs33fniwX"
   },
   "source": [
    "**6.** An RNN expects the input to be `(batch_size, time_steps, and features_per_timestep)` in shape. Hence, we first reshape the padded_docs input into the following format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Vh3tJJR8niwX"
   },
   "outputs": [],
   "source": [
    "padded_docs = padded_docs.reshape(2,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "eUxbcDteniwX",
    "outputId": "718e5f75-5c2c-401d-8fb6-6fa4a7312979"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print(padded_docs[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JouWkvUcniwY"
   },
   "source": [
    "**7.** Define the model—where we are specifying that we will initialize an RNN by using the SimpleRNN method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "TB6M-CwmIJA4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oem/Documents/github/magnimind_hw/magenv/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "embed_length=1\n",
    "max_length=2\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(1,activation='tanh', return_sequences=False,recurrent_initializer='Zeros',\n",
    "                    input_shape=(max_length,embed_length),unroll=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J44_KM0uniwZ"
   },
   "source": [
    "Typically, in a many-to-one task, where there are many inputs (one input in each time step) and outputs, `return_sequences` will be `false`, resulting in the output being obtained only in the final time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UIYuIJiQniwZ"
   },
   "source": [
    "**8.** Connect the RNN output to five nodes of the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "BpeK9HNcniwZ"
   },
   "outputs": [],
   "source": [
    "model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_L1b_wMniwa"
   },
   "source": [
    "We have performed a `Dense(5)`, as there are five possible classes of output (the output of each example has 5 values, where each value corresponds to the probability of it belonging to word `ID 0` to word `ID 4`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QAVKfkMCniwa"
   },
   "source": [
    "**9.** Compile and summarize the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "R1ZSWg4tniwa",
    "outputId": "113b82cd-3334-4961-ce48-20b801d1bafa"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ simple_rnn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ simple_rnn (\u001b[38;5;33mSimpleRNN\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m3\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │            \u001b[38;5;34m10\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13</span> (52.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m13\u001b[0m (52.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13</span> (52.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m13\u001b[0m (52.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PCSRsEywniwa"
   },
   "source": [
    "**10.** Fit the model to predict the output from the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "nb0TC3BTINjA",
    "outputId": "04041a8b-5eb2-4ba4-f09d-3adb8d8bb661"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x72d8caf2e720>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(padded_docs.reshape(2,2,1),np.array(one_hot_encoded_labels),epochs=500, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hmx-lNSTniwb"
   },
   "source": [
    "**11.** Extract prediction on the first input data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "6Oi1ptOpniwb",
    "outputId": "674c3fad-b825-45cb-efd7-5f62d68d20be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3],\n",
       "       [1]], dtype=int32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "eyvN8QPrniwb",
    "outputId": "2fbcf09e-b7c6-4cbf-9c9c-97482792480b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.06338888, 0.06695271, 0.43899867, 0.02413384, 0.4065259 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(padded_docs[0].reshape(1,2,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sp2q16Y_niwb"
   },
   "source": [
    "**12.** Building RNN from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "id": "i-U_R92kIQya",
    "outputId": "dc4f976d-f4b3-49ef-94e1-4ecb05b92288"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasVariable shape=(1, 1), dtype=float32, path=sequential/simple_rnn/simple_rnn_cell/kernel>,\n",
       " <KerasVariable shape=(1, 1), dtype=float32, path=sequential/simple_rnn/simple_rnn_cell/recurrent_kernel>,\n",
       " <KerasVariable shape=(1,), dtype=float32, path=sequential/simple_rnn/simple_rnn_cell/bias>,\n",
       " <KerasVariable shape=(1, 5), dtype=float32, path=sequential/dense/kernel>,\n",
       " <KerasVariable shape=(5,), dtype=float32, path=sequential/dense/bias>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "id": "d-R-YHTTIaHh",
    "outputId": "156b697a-c192-4c10-c03a-c77485ca84c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[1.859338]], dtype=float32),\n",
       " array([[0.3029181]], dtype=float32),\n",
       " array([0.30276027], dtype=float32),\n",
       " array([[-0.23695256, -0.1812719 ,  0.96621007, -1.2200551 ,  0.8559443 ]],\n",
       "       dtype=float32),\n",
       " array([-0.38384706, -0.38403073,  0.36546257, -0.38052478,  0.39729738],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "L2EIKOB6Iuk_",
    "outputId": "5ffc4b99-62da-444a-d537-4d0d3a570507"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.859338]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "fwyfczAYIbsh",
    "outputId": "c4bfb4b0-0daf-4497-a8bd-44929177bb90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.06338888, 0.06695271, 0.43899867, 0.02413384, 0.4065259 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(padded_docs[0].reshape(1,2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "PnI34PY3K3Av",
    "outputId": "c4a0b827-9fa4-4451-fe4f-39bfbb051954"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3],\n",
       "       [1]], dtype=int32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BFV5F3QRIf_a"
   },
   "outputs": [],
   "source": [
    "input_t0 = 3\n",
    "input_t0_kernel_bias = input_t0*model.get_weights()[0] + model.get_weights()[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "GMvs-AoNI10G"
   },
   "outputs": [],
   "source": [
    "hidden_layer0_value = np.tanh(input_t0_kernel_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "AGjDGn7TI9sr"
   },
   "outputs": [],
   "source": [
    "input_t1 = 1\n",
    "input_t1_kernel_bias = input_t1*model.get_weights()[0] + model.get_weights()[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "N6E18qa4JscI"
   },
   "outputs": [],
   "source": [
    "input_t1_recurrent = hidden_layer0_value*model.get_weights()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "fR4aYSWsKChL"
   },
   "outputs": [],
   "source": [
    "total_input_t1 = input_t1_kernel_bias + input_t1_recurrent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "LP2XzgCVKGCx"
   },
   "outputs": [],
   "source": [
    "output_t1 = np.tanh(total_input_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "yeWTORhqKKFN",
    "outputId": "875f6560-060a-45d4-f817-e6d743995b37"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.6173996, -0.5627016,  1.3178085, -1.5830733,  1.2409596]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output = output_t1*model.get_weights()[3] + model.get_weights()[4]\n",
    "final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "WratbnoMKRHG",
    "outputId": "e7b54141-a320-4ca4-8ed7-6151ab965f30"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.06338888, 0.0669527 , 0.4389987 , 0.02413383, 0.40652588]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(final_output)/np.sum(np.exp(final_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jqlDbp-5niwf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (magenv)",
   "language": "python",
   "name": "magenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
